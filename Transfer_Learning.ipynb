{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "do4-UuhLm41r",
        "outputId": "93066201-95e5-4bd1-fde2-0fe59fd9e562"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'yolov5' already exists and is not an empty directory.\n",
            "/content/yolov5\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ultralytics/yolov5\n",
        "%cd yolov5\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNkDc7pEm8wK",
        "outputId": "c0a44e2a-f2b6-41fe-ef55-a78eced55642"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gitpython>=3.1.30 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (3.1.42)\n",
            "Requirement already satisfied: matplotlib>=3.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (1.25.2)\n",
            "Requirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (4.8.0.76)\n",
            "Requirement already satisfied: Pillow>=9.4.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (9.4.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (5.9.5)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (6.0.1)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 12)) (2.31.0)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 13)) (1.11.4)\n",
            "Requirement already satisfied: thop>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 14)) (0.1.1.post2209072238)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 15)) (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 16)) (0.16.0+cu121)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 17)) (4.66.2)\n",
            "Requirement already satisfied: ultralytics>=8.0.232 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 18)) (8.1.19)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 27)) (1.5.3)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 28)) (0.13.1)\n",
            "Requirement already satisfied: setuptools>=65.5.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 42)) (67.7.2)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython>=3.1.30->-r requirements.txt (line 5)) (4.0.11)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (4.49.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (23.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (2.8.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->-r requirements.txt (line 12)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->-r requirements.txt (line 12)) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->-r requirements.txt (line 12)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->-r requirements.txt (line 12)) (2024.2.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (2.1.0)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics>=8.0.232->-r requirements.txt (line 18)) (9.0.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->-r requirements.txt (line 27)) (2023.4)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython>=3.1.30->-r requirements.txt (line 5)) (5.0.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3->-r requirements.txt (line 6)) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->-r requirements.txt (line 15)) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->-r requirements.txt (line 15)) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1. Collect a source video. It may be necessary to divide the video into discrete image frames."
      ],
      "metadata": {
        "id": "ObPN-xlVoU00"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opencv-python-headless  # Use opencv-python-headless for minimal installation\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "from matplotlib import pyplot as plt\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNfShAUooX8J",
        "outputId": "111fb474-3e5b-4786-fd57-8d7b6c5d99f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (4.9.0.80)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python-headless) (1.25.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d337OJT3pRBn",
        "outputId": "a46dc95b-351b-4a39-8a87-6c2fa31c4996"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "\n",
        "def video_to_frames(video_path, frames_dir=\"frames\", every_n_frame=1):\n",
        "    \"\"\"\n",
        "    Extract frames from a video file and print the total number of frames extracted.\n",
        "\n",
        "    Parameters:\n",
        "    - video_path: Path to the video file.\n",
        "    - frames_dir: Directory to save the extracted frames.\n",
        "    - every_n_frame: Extract every nth frame.\n",
        "    \"\"\"\n",
        "    # Ensure the output directory exists\n",
        "    if not os.path.exists(frames_dir):\n",
        "        os.makedirs(frames_dir)\n",
        "\n",
        "    # Open the video file\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frame_count = 0\n",
        "    saved_frame_count = 0  # Initialize a counter for saved frames\n",
        "\n",
        "    while True:\n",
        "        # Read frame\n",
        "        ret, frame = cap.read()\n",
        "\n",
        "        # Break the loop if there are no more frames\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Save every nth frame\n",
        "        if frame_count % every_n_frame == 0:\n",
        "            frame_path = os.path.join(frames_dir, f\"frame_{frame_count}.jpg\")\n",
        "            cv2.imwrite(frame_path, frame)\n",
        "            saved_frame_count += 1  # Increment saved frame counter\n",
        "            print(f\"Frame {frame_count} saved.\")\n",
        "\n",
        "        frame_count += 1\n",
        "\n",
        "    # Release the video capture object\n",
        "    cap.release()\n",
        "    print(f\"Done extracting frames. Total frames saved: {saved_frame_count}\")\n",
        "\n",
        "video_path = '/content/drive/My Drive/video.mp4'\n",
        "video_to_frames(video_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ED2oYY8rnBW_",
        "outputId": "852b88e3-7804-4f19-9106-18261746fa44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frame 0 saved.\n",
            "Frame 1 saved.\n",
            "Frame 2 saved.\n",
            "Frame 3 saved.\n",
            "Frame 4 saved.\n",
            "Frame 5 saved.\n",
            "Frame 6 saved.\n",
            "Frame 7 saved.\n",
            "Frame 8 saved.\n",
            "Frame 9 saved.\n",
            "Frame 10 saved.\n",
            "Frame 11 saved.\n",
            "Frame 12 saved.\n",
            "Frame 13 saved.\n",
            "Frame 14 saved.\n",
            "Frame 15 saved.\n",
            "Frame 16 saved.\n",
            "Frame 17 saved.\n",
            "Frame 18 saved.\n",
            "Frame 19 saved.\n",
            "Frame 20 saved.\n",
            "Frame 21 saved.\n",
            "Frame 22 saved.\n",
            "Frame 23 saved.\n",
            "Frame 24 saved.\n",
            "Frame 25 saved.\n",
            "Frame 26 saved.\n",
            "Frame 27 saved.\n",
            "Frame 28 saved.\n",
            "Frame 29 saved.\n",
            "Frame 30 saved.\n",
            "Frame 31 saved.\n",
            "Frame 32 saved.\n",
            "Frame 33 saved.\n",
            "Frame 34 saved.\n",
            "Frame 35 saved.\n",
            "Frame 36 saved.\n",
            "Frame 37 saved.\n",
            "Frame 38 saved.\n",
            "Frame 39 saved.\n",
            "Frame 40 saved.\n",
            "Frame 41 saved.\n",
            "Frame 42 saved.\n",
            "Frame 43 saved.\n",
            "Frame 44 saved.\n",
            "Frame 45 saved.\n",
            "Frame 46 saved.\n",
            "Frame 47 saved.\n",
            "Frame 48 saved.\n",
            "Frame 49 saved.\n",
            "Frame 50 saved.\n",
            "Frame 51 saved.\n",
            "Frame 52 saved.\n",
            "Frame 53 saved.\n",
            "Frame 54 saved.\n",
            "Frame 55 saved.\n",
            "Frame 56 saved.\n",
            "Frame 57 saved.\n",
            "Frame 58 saved.\n",
            "Frame 59 saved.\n",
            "Frame 60 saved.\n",
            "Frame 61 saved.\n",
            "Frame 62 saved.\n",
            "Frame 63 saved.\n",
            "Frame 64 saved.\n",
            "Frame 65 saved.\n",
            "Frame 66 saved.\n",
            "Frame 67 saved.\n",
            "Frame 68 saved.\n",
            "Frame 69 saved.\n",
            "Frame 70 saved.\n",
            "Frame 71 saved.\n",
            "Frame 72 saved.\n",
            "Frame 73 saved.\n",
            "Frame 74 saved.\n",
            "Frame 75 saved.\n",
            "Frame 76 saved.\n",
            "Frame 77 saved.\n",
            "Frame 78 saved.\n",
            "Frame 79 saved.\n",
            "Frame 80 saved.\n",
            "Frame 81 saved.\n",
            "Frame 82 saved.\n",
            "Frame 83 saved.\n",
            "Frame 84 saved.\n",
            "Frame 85 saved.\n",
            "Frame 86 saved.\n",
            "Frame 87 saved.\n",
            "Frame 88 saved.\n",
            "Frame 89 saved.\n",
            "Frame 90 saved.\n",
            "Frame 91 saved.\n",
            "Frame 92 saved.\n",
            "Frame 93 saved.\n",
            "Frame 94 saved.\n",
            "Frame 95 saved.\n",
            "Frame 96 saved.\n",
            "Frame 97 saved.\n",
            "Frame 98 saved.\n",
            "Frame 99 saved.\n",
            "Frame 100 saved.\n",
            "Frame 101 saved.\n",
            "Frame 102 saved.\n",
            "Frame 103 saved.\n",
            "Frame 104 saved.\n",
            "Frame 105 saved.\n",
            "Frame 106 saved.\n",
            "Frame 107 saved.\n",
            "Frame 108 saved.\n",
            "Frame 109 saved.\n",
            "Frame 110 saved.\n",
            "Frame 111 saved.\n",
            "Frame 112 saved.\n",
            "Frame 113 saved.\n",
            "Frame 114 saved.\n",
            "Frame 115 saved.\n",
            "Frame 116 saved.\n",
            "Frame 117 saved.\n",
            "Frame 118 saved.\n",
            "Frame 119 saved.\n",
            "Frame 120 saved.\n",
            "Frame 121 saved.\n",
            "Frame 122 saved.\n",
            "Frame 123 saved.\n",
            "Frame 124 saved.\n",
            "Frame 125 saved.\n",
            "Frame 126 saved.\n",
            "Frame 127 saved.\n",
            "Frame 128 saved.\n",
            "Frame 129 saved.\n",
            "Frame 130 saved.\n",
            "Frame 131 saved.\n",
            "Frame 132 saved.\n",
            "Frame 133 saved.\n",
            "Frame 134 saved.\n",
            "Frame 135 saved.\n",
            "Frame 136 saved.\n",
            "Frame 137 saved.\n",
            "Frame 138 saved.\n",
            "Frame 139 saved.\n",
            "Frame 140 saved.\n",
            "Frame 141 saved.\n",
            "Frame 142 saved.\n",
            "Frame 143 saved.\n",
            "Frame 144 saved.\n",
            "Frame 145 saved.\n",
            "Frame 146 saved.\n",
            "Frame 147 saved.\n",
            "Frame 148 saved.\n",
            "Frame 149 saved.\n",
            "Frame 150 saved.\n",
            "Frame 151 saved.\n",
            "Frame 152 saved.\n",
            "Frame 153 saved.\n",
            "Frame 154 saved.\n",
            "Frame 155 saved.\n",
            "Frame 156 saved.\n",
            "Frame 157 saved.\n",
            "Frame 158 saved.\n",
            "Frame 159 saved.\n",
            "Frame 160 saved.\n",
            "Frame 161 saved.\n",
            "Frame 162 saved.\n",
            "Frame 163 saved.\n",
            "Frame 164 saved.\n",
            "Frame 165 saved.\n",
            "Frame 166 saved.\n",
            "Frame 167 saved.\n",
            "Frame 168 saved.\n",
            "Frame 169 saved.\n",
            "Frame 170 saved.\n",
            "Frame 171 saved.\n",
            "Frame 172 saved.\n",
            "Frame 173 saved.\n",
            "Frame 174 saved.\n",
            "Frame 175 saved.\n",
            "Frame 176 saved.\n",
            "Frame 177 saved.\n",
            "Frame 178 saved.\n",
            "Frame 179 saved.\n",
            "Frame 180 saved.\n",
            "Frame 181 saved.\n",
            "Frame 182 saved.\n",
            "Frame 183 saved.\n",
            "Frame 184 saved.\n",
            "Frame 185 saved.\n",
            "Frame 186 saved.\n",
            "Frame 187 saved.\n",
            "Frame 188 saved.\n",
            "Frame 189 saved.\n",
            "Frame 190 saved.\n",
            "Frame 191 saved.\n",
            "Frame 192 saved.\n",
            "Frame 193 saved.\n",
            "Frame 194 saved.\n",
            "Frame 195 saved.\n",
            "Frame 196 saved.\n",
            "Frame 197 saved.\n",
            "Frame 198 saved.\n",
            "Frame 199 saved.\n",
            "Frame 200 saved.\n",
            "Frame 201 saved.\n",
            "Frame 202 saved.\n",
            "Frame 203 saved.\n",
            "Frame 204 saved.\n",
            "Frame 205 saved.\n",
            "Frame 206 saved.\n",
            "Frame 207 saved.\n",
            "Frame 208 saved.\n",
            "Frame 209 saved.\n",
            "Frame 210 saved.\n",
            "Frame 211 saved.\n",
            "Frame 212 saved.\n",
            "Frame 213 saved.\n",
            "Frame 214 saved.\n",
            "Frame 215 saved.\n",
            "Frame 216 saved.\n",
            "Frame 217 saved.\n",
            "Done extracting frames. Total frames saved: 218\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2. Conduct inference on each frame of the video, drawing bounding boxes around detected vehicles."
      ],
      "metadata": {
        "id": "TkvhMMaGqEFa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def process_and_display_frame(frame, model):\n",
        "    # Convert frame to RGB as YOLOv5 expects RGB images\n",
        "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Perform inference\n",
        "    results = model(frame_rgb)\n",
        "\n",
        "    # Desired classes telling about what labels to be included\n",
        "    desired_classes = ['car', 'truck', 'bus', 'traffic light', 'person']\n",
        "\n",
        "    # Draw bounding boxes on the original frame\n",
        "    for *xyxy, conf, cls in results.xyxy[0]:  # results.xyxy[0] contains bbox coords, confidence, class\n",
        "        label = model.names[int(cls)]  # Get the class name using the class index\n",
        "        if label in desired_classes:  # Check against updated list of desired classes\n",
        "            start_point = (int(xyxy[0]), int(xyxy[1]))\n",
        "            end_point = (int(xyxy[2]), int(xyxy[3]))\n",
        "            color = (255, 0, 0)  # Box color\n",
        "            frame = cv2.rectangle(frame, start_point, end_point, color, 2)\n",
        "            cv2.putText(frame, label, (int(xyxy[0]), int(xyxy[1]-10)), cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)\n",
        "\n",
        "    # Display the frame\n",
        "    cv2.imshow('Frame', frame)\n",
        "    cv2.waitKey(1)  # Use cv2.waitKey(0) if you want to display each frame until a key is pressed\n"
      ],
      "metadata": {
        "id": "CkcHyrfbomzU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Load the YOLOv5 model\n",
        "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
        "\n",
        "\n",
        "cap = cv2.VideoCapture('video.mp4')\n",
        "\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break  # Break the loop if no frame is returned\n",
        "\n",
        "    process_and_display_frame(frame, model)\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpQn2u9vqOXp",
        "outputId": "ee2d5d94-78da-49bd-810e-42ef6bfd130b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/ultralytics_yolov5_master\n",
            "YOLOv5 ðŸš€ 2024-2-28 Python-3.10.12 torch-2.1.0+cu121 CPU\n",
            "\n",
            "Fusing layers... \n",
            "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
            "Adding AutoShape... \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3. Format the results back into a video.\n",
        "\n"
      ],
      "metadata": {
        "id": "NUBvRVIWqxZW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import torch\n",
        "from google.colab.patches import cv2_imshow  # For displaying frames within Colab\n",
        "\n",
        "# Load the YOLOv5 model\n",
        "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
        "\n",
        "def process_and_display_frame(frame, model):\n",
        "    \"\"\"\n",
        "    Process a single frame through YOLOv5 model, draw bounding boxes on detected objects of interest,\n",
        "    and return the processed frame.\n",
        "    \"\"\"\n",
        "    # Convert frame to RGB (YOLOv5 expects RGB images)\n",
        "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Perform inference\n",
        "    results = model(frame_rgb)\n",
        "\n",
        "    # Convert results to numpy array and draw bounding boxes\n",
        "    labels, cord = results.xyxyn[0][:, -1].numpy(), results.xyxyn[0][:, :-1].numpy()\n",
        "    n = len(labels)\n",
        "    for i in range(n):\n",
        "        row = cord[i]\n",
        "        desired_class = model.names[int(labels[i])]\n",
        "        # Check if the detected class is one of the specified types\n",
        "        if desired_class in ['car', 'truck', 'bus', 'traffic light', 'person']:\n",
        "            x1, y1, x2, y2, conf = int(row[0]*frame_width), int(row[1]*frame_height), \\\n",
        "                                   int(row[2]*frame_width), int(row[3]*frame_height), row[4]\n",
        "            # Draw rectangle and label\n",
        "            cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
        "            cv2.putText(frame, f'{desired_class} {int(conf * 100)}%',\n",
        "                        (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 2)\n",
        "    return frame\n",
        "\n",
        "# Initialize video capture\n",
        "cap = cv2.VideoCapture('/content/drive/My Drive/video.mp4')\n",
        "frame_width = int(cap.get(3))\n",
        "frame_height = int(cap.get(4))\n",
        "\n",
        "# Define the codec and create VideoWriter object\n",
        "out = cv2.VideoWriter('Output_Video_1.avi', cv2.VideoWriter_fourcc('M','J','P','G'), 30, (frame_width, frame_height))\n",
        "\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break  # No more frames, exit the loop\n",
        "\n",
        "    # Process the frame\n",
        "    processed_frame = process_and_display_frame(frame, model)\n",
        "\n",
        "    # Write the processed frame to the output video\n",
        "    out.write(processed_frame)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2b7bvV5rCuB",
        "outputId": "fd659b19-8138-4cbc-c0bd-1b1f03d2b515"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/ultralytics_yolov5_master\n",
            "YOLOv5 ðŸš€ 2024-2-28 Python-3.10.12 torch-2.1.0+cu121 CPU\n",
            "\n",
            "Fusing layers... \n",
            "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
            "Adding AutoShape... \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3. Format the results back into a video.\n",
        "\n"
      ],
      "metadata": {
        "id": "EIfQEBl5uA3e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Release everything if job is finished\n",
        "cap.release()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "oQ2XfqrcyFml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download('Output_Video_1.avi')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "NAWxXIHWtoL4",
        "outputId": "8fbcd670-94ab-42e2-9ae9-d745e70b84df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_75c7d932-fe79-4885-a6e0-c45b134a5fac\", \"Output_Video_1.avi\", 52755560)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8RHvR_fqNA8M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
